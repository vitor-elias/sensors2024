{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import argparse\n",
    "import torch\n",
    "import optuna\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import Linear, ReLU, Sequential, LayerNorm\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import sensors.nn.models as models\n",
    "import sensors.utils.utils as utils\n",
    "import sensors.utils.fault_detection as fd\n",
    "\n",
    "from sensors.utils.utils import roc_params, compute_auc\n",
    "\n",
    "from importlib import reload\n",
    "models = reload(models)\n",
    "utils = reload(utils)\n",
    "\n",
    "from pyprojroot import here\n",
    "root_dir = str(here())\n",
    "\n",
    "data_dir = '~/data/interim/'\n",
    "\n",
    "matplotlib.rcParams.update({'font.size': 20})\n",
    "matplotlib.rcParams.update({'font.family': 'DejaVu Serif'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_encoding_layers = 4\n",
    "\n",
    "mlp_in = 100\n",
    "reduction = 0.75\n",
    "\n",
    "mlp_in_list = []\n",
    "mlp_in_list.append(mlp_in)\n",
    "\n",
    "# Encoder\n",
    "encoder_layers = OrderedDict()\n",
    "encoder_layers['linear_0'] = Linear(mlp_in, round( mlp_in * reduction ))\n",
    "encoder_layers['relu_0'] = ReLU()\n",
    "\n",
    "for n in range(1, n_encoding_layers):\n",
    "    mlp_in = round( mlp_in * reduction )\n",
    "    mlp_in_list.append(mlp_in)\n",
    "\n",
    "    encoder_layers[f'linear_{n}'] = Linear(mlp_in, round( mlp_in * reduction ))\n",
    "    encoder_layers[f'relu_{n}'] = ReLU()\n",
    "\n",
    "mlp_in_list.append( round(mlp_in * reduction ))\n",
    "encoder = Sequential(encoder_layers)\n",
    "\n",
    "# Decoder\n",
    "\n",
    "decoder_layers = OrderedDict()\n",
    "for n in range(0, n_encoding_layers):\n",
    "    decoder_layers[f'linear_{n}'] = Linear( mlp_in_list[-n-1], mlp_in_list[-n-2] )\n",
    "    decoder_layers[f'relu_{n}'] = ReLU()\n",
    "\n",
    "decoder = Sequential(decoder_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X is your 2D matrix with shape (500, 100)\n",
    "X = torch.randn(500, 100)\n",
    "\n",
    "# Define the autoencoder\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(100, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 25),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(25, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 100),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the autoencoder and the optimizer\n",
    "model = Autoencoder()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Create a DataLoader\n",
    "dataset = TensorDataset(X, X)  # we want to reconstruct the same input\n",
    "dataloader = DataLoader(dataset, batch_size=20, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):  # number of epochs\n",
    "    for batch_features, _ in dataloader:\n",
    "        # Forward pass\n",
    "        outputs = model(batch_features)\n",
    "        loss = criterion(outputs, batch_features)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, 100, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rng_seed = 0\n",
    "# torch.manual_seed(rng_seed)\n",
    "# torch.cuda.manual_seed(rng_seed)\n",
    "# np.random.seed(rng_seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "dataset = 'df_StOlavs_D1L2B'\n",
    "df_orig = pd.read_parquet(data_dir + f'{dataset}.parq')\n",
    "\n",
    "df, nodes = fd.treat_nodes(df_orig)\n",
    "_, nodes['subgraph'] = fd.NNGraph(nodes, radius=15, subgraphs=True)\n",
    "\n",
    "main_graph = nodes.subgraph.value_counts().index[0]\n",
    "nodes = nodes.query('subgraph==@main_graph').copy()\n",
    "G = fd.NNGraph(nodes, radius=15)\n",
    "df = df[df.pid.isin(nodes.pid.unique())].copy()   \n",
    "\n",
    "data, labels, data_dfs = utils.generate_cluster_anomaly(df, nodes, G, data_size=1)\n",
    "\n",
    "n_timestamps = data.shape[2]\n",
    "\n",
    "# Possible hyperparameters\n",
    "n_clusters = 10\n",
    "n_extra_feats = 0 #########################\n",
    "conv1d_n_feats = 3\n",
    "conv1d_kernel_size = 60\n",
    "conv1d_stride = 30\n",
    "graphconv_n_feats = 30\n",
    "\n",
    "n_encoding_layers = 2\n",
    "reduction = 0.5\n",
    "\n",
    "N_epochs = 10000\n",
    "batch_size = 256\n",
    "\n",
    "weight_loss = 1\n",
    "weight_coords = 0.25\n",
    "\n",
    "model = models.Autoencoder(conv1d_n_feats, conv1d_kernel_size, conv1d_stride, \n",
    "                n_timestamps, n_encoding_layers, reduction)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "for i in range(data.shape[0]):\n",
    "    print(i)\n",
    "    X = torch.tensor(data[i,:,:]).float().to(device)\n",
    "    norm_X = LayerNorm(X.shape, elementwise_affine=False) # Normalizes the <entire matrix> to 0 mean 1 var\n",
    "    X = norm_X(X)\n",
    "    \n",
    "    label = labels[i,:]\n",
    "\n",
    "    dataset = TensorDataset(X, X)  # we want to reconstruct the same input\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    loss_values = []\n",
    "\n",
    "    model.train()\n",
    "    model.reset_parameters()\n",
    "    for epoch in tqdm(range(N_epochs)):\n",
    "        for batch, _ in dataloader:\n",
    "            \n",
    "            # batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch)\n",
    "            loss = loss_function(batch, output)\n",
    "            loss_values.append(loss.cpu().detach().numpy())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "plt.plot(loss_values)\n",
    "plt.show()\n",
    "\n",
    "model.eval()\n",
    "Y = model(X)\n",
    "\n",
    "f = nn.MSELoss(reduction='none')\n",
    "score = torch.mean(f(X,Y), axis=1).cpu().detach().numpy()\n",
    "\n",
    "nodes['anomaly'] = label\n",
    "nodes['score'] = score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.nn.MSELoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cmap = ListedColormap(plt.cm.viridis(np.linspace(0,1,2)))\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(16,5))\n",
    "plotting_params = {'edge_color':'darkgray', 'edge_width':1.5,'vertex_color':'black', 'vertex_size':50}\n",
    "G.plotting.update(plotting_params)\n",
    "G.plot_signal(label, ax=ax[0], plot_name='Label')\n",
    "\n",
    "ax[0].collections[0].set_cmap(label_cmap)  # Modify the colormap of the plotted data\n",
    "ax[0].axis('off')\n",
    "\n",
    "G.plot_signal(score, ax=ax[1], plot_name='Anomaly Score')\n",
    "ax[1].collections[0].set_cmap('viridis')\n",
    "ax[1].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.visualize_map(nodes, color='anomaly', size=np.ones(nodes.pid.nunique()), size_max=10, title='Label',\n",
    "                     hover_data=['cluster'], zoom=15, figsize=(600,600), colormap='viridis')\n",
    "                    \n",
    "utils.visualize_map(nodes, color='score', size=np.ones(nodes.pid.nunique()), size_max=10, title='Anomaly score',\n",
    "                    hover_data=['cluster'], zoom=15, figsize=(600,600), colormap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr, fpr, _ = roc_params(metric=score, label=label, interp=True)\n",
    "auc = compute_auc(tpr,fpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# study = joblib.load(root_dir+'/outputs/log_20parts_10level_1anomaly.pkl')\n",
    "# study_b = joblib.load(root_dir+'/outputs/log_20parts_10level_1anomaly_b.pkl')\n",
    "# df_study = pd.concat([study.trials_dataframe(), study_b.trials_dataframe()])\n",
    "\n",
    "study = joblib.load(root_dir+'/outputs/log_20parts_10.0level_1anomaly_nokmeans.pkl')\n",
    "df_study = study.trials_dataframe().drop(columns=['datetime_start', 'datetime_complete', 'duration'])\n",
    "\n",
    "params = list(study.best_params.keys())\n",
    "df_study.columns = df_study.columns.str.replace(\"user_attrs_\", \"\")\n",
    "df_study.columns = df_study.columns.str.replace(\"params_\", \"\")\n",
    "\n",
    "columns_to_show = params\n",
    "columns_to_show.extend(['number', 'state',\n",
    "                        'value_cscore', 'std_cscore', 'min_cscore', 'mean_auc', 'std_auc',\n",
    "                        # 'cscore_list', 'auc_list'\n",
    "                        ])\n",
    "\n",
    "df_study = df_study[columns_to_show]\n",
    "df_study.sort_values('value_cscore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cluster(N_epochs, model, X, G, device, weight_loss=0.25, lr=1e-3):\n",
    " \n",
    "    loss_evo = []\n",
    "    loss_mc_evo = []\n",
    "    loss_o_evo = []\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    X = X.to(device)\n",
    "\n",
    "    # Node coordinates\n",
    "    C = torch.tensor(G.coords)\n",
    "    A = torch.tensor(G.W.toarray()).float() #Using W as a float() tensor\n",
    "    A = A.to(device)    \n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # num_clusters_per_feature = [5, 6]\n",
    "    # kmeans_feats = cluster.kmeans_features(C, num_clusters_per_feature).to(device).float()\n",
    "\n",
    "    kmeans_feats = None\n",
    "\n",
    "    model.train()\n",
    "    model.reset_parameters()\n",
    "    for epoch in tqdm(range(N_epochs)):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        S, loss_mc, loss_o = model(X, A, kmeans_feats)\n",
    "        loss = loss_mc + weight_loss*loss_o\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_evo.append(loss.item())\n",
    "        loss_mc_evo.append(loss_mc.item())\n",
    "        loss_o_evo.append(loss_o.item())\n",
    "\n",
    "    return S\n",
    "\n",
    "def evaluate_model(model, weight_loss, N_epochs, data, labels, data_dfs, G, nodes_orig, device):\n",
    "\n",
    "    cluster_score_list = []\n",
    "    auc_list = []\n",
    "    S_list = []\n",
    "    nodes_list = []\n",
    "    for i in range(data.shape[0]):\n",
    "        X = data[i,:,:]\n",
    "        label = labels[i,:]\n",
    "        df_anomaly = data_dfs[i]\n",
    "\n",
    "        nodes = nodes_orig.copy()\n",
    "\n",
    "        S = train_cluster(N_epochs, model, X, G, device, weight_loss)\n",
    "        S_list.append(S)\n",
    "\n",
    "        cluster_score, auc, nodes = get_score(nodes, df_anomaly, S)\n",
    "        nodes_list.append(nodes)\n",
    "        \n",
    "        cluster_score_list.append(cluster_score)\n",
    "        auc_list.append(auc)\n",
    "    \n",
    "    return cluster_score_list, auc_list, S_list, nodes_list\n",
    "\n",
    "def get_score(nodes, df_anomaly, S):\n",
    "\n",
    "    nodes['pred'] = S.argmax(dim=1).cpu().numpy()\n",
    "    nodes['score'] = S.softmax(dim=-1).detach().cpu().numpy().max(axis=1)\n",
    "    nodes['anomaly'] = df_anomaly[['pid','anomaly']].groupby('pid').anomaly.max().values\n",
    "\n",
    "    most_common_preds = nodes.query('anomaly!=0').groupby('anomaly')['pred'].apply(lambda x: x.mode()[0])\n",
    "\n",
    "    nodes['new_pred'] = nodes['pred']\n",
    "    nodes.loc[~nodes.pred.isin(most_common_preds.values),'new_pred'] = -1\n",
    "\n",
    "    max_anomaly = nodes.groupby('new_pred')['anomaly'].transform('max')\n",
    "    nodes.loc[nodes['new_pred'] != -1, 'new_pred'] = max_anomaly\n",
    "    nodes.loc[nodes['new_pred'] == -1, 'new_pred'] = 0\n",
    "\n",
    "    average = 'binary' if df_anomaly.anomaly.nunique()==2 else 'weighted'\n",
    "    cluster_score = f1_score(y_true=nodes.anomaly, y_pred=nodes.new_pred, average=average)\n",
    "\n",
    "    tpr, fpr, _ = roc_params(metric=nodes.score, label=(nodes.anomaly>0), interp=True)\n",
    "    auc = compute_auc(tpr,fpr)\n",
    "\n",
    "    return cluster_score, auc, nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# epochs_list = [100, 250, 500, 750, 1000, 1500, 2000, 3000, 5000, 7500, 10000]\n",
    "epochs_list = [500, 1000]\n",
    "cscore_results = []\n",
    "auc_results = []\n",
    "S_results = []\n",
    "nodes_results = []\n",
    "\n",
    "for N_epochs in epochs_list:\n",
    "\n",
    "    print(N_epochs)\n",
    "\n",
    "    MANUAL_SEED = 0\n",
    "    torch.manual_seed(MANUAL_SEED)  # set seed (manually) for generate random numbers with CPU\n",
    "    torch.cuda.manual_seed(MANUAL_SEED)  # set seed (manually) for generate random numbers with GPU --> \"CUDA = GPU\"\n",
    "    np.random.seed(MANUAL_SEED)\n",
    "\n",
    "\n",
    "    # OBTAINING DATA\n",
    "    dataset = 'df_StOlavs_D1L2B'\n",
    "    df_orig = pd.read_parquet(data_dir + f'{dataset}.parq')\n",
    "\n",
    "    df, nodes = fd.treat_nodes(df_orig)\n",
    "    _, nodes['subgraph'] = fd.NNGraph(nodes, radius=15, subgraphs=True)\n",
    "\n",
    "    main_graph = nodes.subgraph.value_counts().index[0]\n",
    "    nodes = nodes.query('subgraph==@main_graph').copy()\n",
    "    G = fd.NNGraph(nodes, radius=15)\n",
    "    df = df[df.pid.isin(nodes.pid.unique())].copy()   \n",
    "\n",
    "    data, labels, data_dfs = utils.generate_cluster_anomaly(df, nodes, G, data_size=25)\n",
    "\n",
    "    n_timestamps = data.shape[2]\n",
    "\n",
    "    # Possible hyperparameters\n",
    "    n_clusters = 10\n",
    "    n_extra_feats = 0 #########################\n",
    "    conv1d_n_feats = 3\n",
    "    conv1d_kernel_size = 60\n",
    "    conv1d_stride = 30\n",
    "    graphconv_n_feats = 30\n",
    "\n",
    "    weight_loss = 1\n",
    "    weight_coords = 0.25\n",
    "\n",
    "    model = models.ClusterTS(conv1d_n_feats, conv1d_kernel_size, conv1d_stride, graphconv_n_feats,\n",
    "                    n_timestamps, n_clusters, n_extra_feats, weight_coords)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # np.random.seed(32)\n",
    "    cluster_score_list, auc_list, S_list, nodes_list = evaluate_model(model, weight_loss, N_epochs, data, labels, data_dfs, G, nodes, device)\n",
    "    \n",
    "    cscore_results.append(cluster_score_list)\n",
    "    auc_results.append(auc_list)\n",
    "    S_results.append(S_list)\n",
    "    nodes_results.append(nodes_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs_list = [100, 250, 500, 750, 1000, 1500, 2000, 3000, 5000, 7500, 10000]\n",
    "\n",
    "outfile = 'testing_epochs_nokmeans.pkl'\n",
    "# Save variables\n",
    "with open('../../outputs/pickles/' + outfile, 'wb') as f:\n",
    "    pickle.dump([epochs_list, cscore_results, auc_results, S_results, nodes_results], f)\n",
    "\n",
    "# Load variables\n",
    "with open('../../outputs/pickles/' + outfile, 'rb') as f:\n",
    "    epochs_list, cscore_results, auc_results, S_results, nodes_results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(x=epochs_list, y=[np.mean(l) for l in cscore_results], width=700).show()\n",
    "px.line(x=epochs_list, y=[np.mean(l) for l in auc_results], width=700).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_epoch = 1\n",
    "S_list = S_results[index_epoch]\n",
    "nodes_list = nodes_results[index_epoch]\n",
    "\n",
    "index = 3\n",
    "S = S_list[index]\n",
    "nodes = nodes_list[index]\n",
    "label = (nodes.anomaly>0).values\n",
    "\n",
    "print(f'Epochs: {epochs_list[index_epoch]}, case: {index}')\n",
    "print(f'Clustering score: {cscore_results[index_epoch][index]}')\n",
    "print(f'AUC: {auc_results[index_epoch][index]}')\n",
    "\n",
    "label_cmap = ListedColormap(plt.cm.viridis(np.linspace(0,1,nodes.anomaly.nunique())))\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(16,5))\n",
    "plotting_params = {'edge_color':'darkgray', 'edge_width':1.5,'vertex_color':'black', 'vertex_size':50}\n",
    "G.plotting.update(plotting_params)\n",
    "G.plot_signal(label, ax=ax[0], plot_name='Label')\n",
    "\n",
    "ax[0].collections[0].set_cmap(label_cmap)  # Modify the colormap of the plotted data\n",
    "ax[0].axis('off')\n",
    "\n",
    "G.plot_signal(np.array(S.argmax(dim=1).cpu()), ax=ax[1], plot_name='Clustering')\n",
    "ax[1].collections[0].set_cmap('viridis')\n",
    "ax[1].axis('off')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(16,5))\n",
    "plotting_params = {'edge_color':'darkgray', 'edge_width':1.5,'vertex_color':'black', 'vertex_size':50}\n",
    "G.plotting.update(plotting_params)\n",
    "G.plot_signal(nodes.new_pred.values, ax=ax[0], plot_name='Anomaly-adjusted Clustering')\n",
    "\n",
    "ax[0].collections[0].set_cmap(label_cmap)  # Modify the colormap of the plotted data\n",
    "ax[0].axis('off')\n",
    "\n",
    "G.plot_signal(S.softmax(dim=-1).detach().cpu().numpy().max(axis=1), ax=ax[1], plot_name='Anomaly Score')\n",
    "ax[1].collections[0].set_cmap('viridis')\n",
    "ax[1].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.visualize_map(nodes, color='anomaly', size=np.ones(nodes.pid.nunique()), size_max=10, title='Label',\n",
    "                     hover_data=['cluster'], zoom=15, figsize=(600,600), colormap='viridis')\n",
    "                    \n",
    "utils.visualize_map(nodes, color='pred', size=np.ones(nodes.pid.nunique()), size_max=10, title='Clustering',\n",
    "                    hover_data=['cluster'], zoom=15, figsize=(600,600), colormap='viridis')\n",
    "\n",
    "utils.visualize_map(nodes, color='new_pred', size=np.ones(nodes.pid.nunique()), size_max=10, title='Anomaly-adjusted Clustering',\n",
    "                     hover_data=['anomaly'], zoom=15, figsize=(600,600), colormap='viridis')\n",
    "                    \n",
    "utils.visualize_map(nodes, color='score', size=np.ones(nodes.pid.nunique()), size_max=10, title='Anomaly score',\n",
    "                    hover_data=['cluster'], zoom=15, figsize=(600,600), colormap='viridis')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sensors-BoU2skHt-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
