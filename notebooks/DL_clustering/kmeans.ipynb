{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import scipy as sp\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import pygsp\n",
    "import optuna\n",
    "import joblib\n",
    "import gc\n",
    "import argparse\n",
    "import os\n",
    "import matplotlib\n",
    "import pickle\n",
    "import glob\n",
    "\n",
    "from matplotlib.ticker import ScalarFormatter, StrMethodFormatter, FormatStrFormatter, FuncFormatter\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import mean_squared_error, confusion_matrix, auc, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from optuna.samplers import TPESampler, BruteForceSampler\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import Linear, Conv1d, LayerNorm, DataParallel\n",
    "from torch_geometric.nn import GCNConv, Sequential, GraphConv\n",
    "from torch_geometric.nn.dense import mincut_pool, dense_mincut_pool\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "from torch.nn.functional import glu\n",
    "\n",
    "from torch_geometric.nn.models import GraphUNet\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_networkx, grid\n",
    "\n",
    "import sensors.utils.fault_detection as fd\n",
    "import sensors.utils.analysis as ana\n",
    "import sensors.utils.utils as utils\n",
    "\n",
    "from importlib import reload\n",
    "ana = reload(ana)\n",
    "utils = reload(utils)\n",
    "\n",
    "\n",
    "from pyprojroot import here\n",
    "ROOT_DIR = str(here())\n",
    "data_dir = '~/data/interim/'\n",
    "\n",
    "matplotlib.rcParams.update({'font.size': 20})\n",
    "matplotlib.rcParams.update({'font.family': 'DejaVu Serif'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'df_StOlavs_D1L2B'\n",
    "df_orig = pd.read_parquet(data_dir + f'{dataset}.parq')\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "df, nodes = fd.treat_nodes(df_orig)\n",
    "G, nodes['subgraph'] = fd.NNGraph(nodes, radius=15, subgraphs=True)\n",
    "\n",
    "main_graph = nodes.subgraph.value_counts().index[0]\n",
    "nodes = nodes.query('subgraph==@main_graph')\n",
    "G = fd.NNGraph(nodes, radius=15)\n",
    "df = df[df.pid.isin(nodes.pid.unique())]\n",
    "\n",
    "nodes['cluster'] = KMeans(n_clusters=20, n_init='auto').fit_predict(nodes[['northing','easting']])\n",
    "df.drop('cluster', axis=1, inplace=True)\n",
    "df = df.merge(nodes[['pid','cluster']], how='left', on='pid')\n",
    "df['strcluster'] = df.cluster.astype(str).values\n",
    "\n",
    "df_anomaly = df.copy()\n",
    "df_anomaly['anomaly'] = 0\n",
    "\n",
    "# # Anomaly test - different anomalous clusters\n",
    "# # Anomaly 1\n",
    "# anomaly_sensor = (df_anomaly.cluster==0)\n",
    "# anomaly_onset = (df_anomaly.timestamp>'Jul 2020')&(df_anomaly.timestamp<'Jan 2021')\n",
    "# anomaly_loc = anomaly_sensor&anomaly_onset\n",
    "\n",
    "# df_anomaly.loc[anomaly_loc, 'smoothed'] += 0\n",
    "# df_anomaly.loc[anomaly_loc, 'anomaly'] = 1\n",
    "\n",
    "# Anomaly 2\n",
    "anomaly_sensor = (df_anomaly.cluster==12)\n",
    "anomaly_onset = (df_anomaly.timestamp>'Jul 2020')\n",
    "anomaly_loc = anomaly_sensor&anomaly_onset\n",
    "\n",
    "df_anomaly.loc[anomaly_loc, 'smoothed'] += 5\n",
    "df_anomaly.loc[anomaly_loc, 'anomaly'] = 2\n",
    "\n",
    "# Anomaly 3\n",
    "anomaly_sensor = (df_anomaly.cluster==19)\n",
    "anomaly_onset = (df_anomaly.timestamp>'Jul 2020')\n",
    "anomaly_loc = anomaly_sensor&anomaly_onset\n",
    "\n",
    "df_anomaly.loc[anomaly_loc, 'smoothed'] -= 5\n",
    "df_anomaly.loc[anomaly_loc, 'anomaly'] = 3\n",
    "\n",
    "X = df_anomaly.pivot(index='pid', columns='timestamp', values='smoothed').values\n",
    "X = torch.tensor(X)\n",
    "\n",
    "label = df_anomaly.pivot(index='pid', columns='timestamp', values='anomaly').values.max(axis=1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "G.plot_signal(label, ax=ax, plot_name='')\n",
    "\n",
    "# label_cmap = ListedColormap(['blue','goldenrod', 'sienna','aquamarine'])\n",
    "label_cmap = ListedColormap(plt.cm.viridis(np.linspace(0,1,df_anomaly.anomaly.nunique())))\n",
    "\n",
    "ax.collections[0].set_cmap(label_cmap)  # Modify the colormap of the plotted data\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of clusters for each feature\n",
    "def kmeans_features(data, num_clusters):\n",
    "\n",
    "    def cluster_kmeans(tensor, k):\n",
    "        kmeans = KMeans(n_clusters=k, n_init=1)\n",
    "        kmeans.fit(tensor)\n",
    "        return kmeans.labels_\n",
    "\n",
    "    kmeans_features = []\n",
    "    # Perform clustering for each number of clusters\n",
    "    for k in num_clusters:\n",
    "        # Perform K-means clustering\n",
    "        cluster_labels = cluster_kmeans(data, k)\n",
    "        kmeans_features.append(cluster_labels)\n",
    "\n",
    "    return torch.tensor(np.array(kmeans_features).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusterTS(nn.Module):\n",
    "    def __init__(self,\n",
    "                 conv1d_n_feats, conv1d_kernel_size, conv1d_stride,\n",
    "                 graphconv_n_feats,\n",
    "                 n_timestamps,\n",
    "                 n_clusters,\n",
    "                 n_extra_feats):\n",
    "        \n",
    "        super(ClusterTS, self).__init__()\n",
    "        self.conv1d = nn.Conv1d(in_channels=1, out_channels=conv1d_n_feats,\n",
    "                                kernel_size=conv1d_kernel_size, stride=conv1d_stride)\n",
    "        \n",
    "        self.L_in = n_timestamps\n",
    "        self.L_out = math.floor((self.L_in - conv1d_kernel_size)/conv1d_stride + 1)\n",
    "\n",
    "        self.conv1d_out = conv1d_n_feats*self.L_out\n",
    "        \n",
    "        mlp_in = self.conv1d_out + n_extra_feats\n",
    "        self.mcp_mlp = Linear(mlp_in, n_clusters)\n",
    "    \n",
    "    def forward(self, X, A, extra_feats=None):\n",
    "\n",
    "        #HP\n",
    "        factor_coords = 0.5\n",
    "\n",
    "        # Data\n",
    "        X = X.float()\n",
    "        norm_X = LayerNorm(X.shape, elementwise_affine=False)\n",
    "        X = norm_X(X)\n",
    "\n",
    "        X = X.unsqueeze(1) # adjusting shape for conv1d\n",
    "        X = self.conv1d(X)\n",
    "\n",
    "        X = X.reshape((X.shape[0],-1)) #\n",
    "\n",
    "        if extra_feats is not None:\n",
    "            norm_f = LayerNorm(extra_feats.shape, elementwise_affine=False)\n",
    "            extra_feats = factor_coords*norm_f(extra_feats)\n",
    "            X = torch.cat((X,extra_feats),dim=1)\n",
    "\n",
    "        S = self.mcp_mlp(X)\n",
    "\n",
    "        _, _, loss_mc, loss_o = dense_mincut_pool(X, A, S)\n",
    "\n",
    "        # return torch.softmax(S, dim=-1), loss_mc, loss_o\n",
    "        return S, loss_mc, loss_o, X, C\n",
    "    \n",
    "\n",
    "# torch.random.manual_seed(0)\n",
    "\n",
    "conv1d_n_feats = 3\n",
    "conv1d_kernel_size = 60\n",
    "conv1d_stride = 30\n",
    "\n",
    "graphconv_n_feats = 30\n",
    "\n",
    "n_nodes = X.shape[0]\n",
    "n_timestamps = X.shape[1]\n",
    "\n",
    "n_clusters = 20\n",
    "factor = 0.25\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "X = X.to(device)\n",
    "\n",
    "# Node coordinates\n",
    "C = torch.tensor(G.coords)\n",
    "A = torch.tensor(G.W.toarray()).float() #Using W as a float() tensor\n",
    "A = A.to(device)\n",
    "\n",
    "num_clusters_per_feature = [5, 6]\n",
    "kmeans_feats = kmeans_features(C, num_clusters_per_feature).to(device).float()\n",
    "n_extra_feats = kmeans_feats.shape[1]\n",
    "\n",
    "# kmeans_feats = None\n",
    "# n_extra_feats = 0\n",
    "\n",
    "model = ClusterTS(conv1d_n_feats, conv1d_kernel_size, conv1d_stride, graphconv_n_feats,\n",
    "                  n_timestamps, n_clusters, n_extra_feats)\n",
    "# model.conv1d = DataParallel(model.conv1d)\n",
    "# model.mcp_mlp = DataParallel(model.mcp_mlp)\n",
    "model = model.to(device)\n",
    "\n",
    "print(f'Clusters: {n_clusters}')\n",
    "print(f'Factor: {factor}')\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def train(N_epochs):\n",
    " \n",
    "    loss_evo = []\n",
    "    loss_mc_evo = []\n",
    "    loss_o_evo = []\n",
    "\n",
    "    model.train()\n",
    "    for epoch in tqdm(range(N_epochs)):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        _, loss_mc, loss_o, Xm, Cm = model(X, A, kmeans_feats)\n",
    "        loss = loss_mc + factor*loss_o\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_evo.append(loss.item())\n",
    "        loss_mc_evo.append(loss_mc.item())\n",
    "        loss_o_evo.append(loss_o.item())\n",
    "\n",
    "    return loss_evo, loss_mc_evo, loss_o_evo\n",
    "\n",
    "\n",
    "loss_evo, loss_mc_evo, loss_o_evo = train(10000)\n",
    "\n",
    "model.eval()\n",
    "S, _, _, Xm, Cm = model(X, A, kmeans_feats)\n",
    "S = S.cpu()\n",
    "\n",
    "\n",
    "print(model)\n",
    "\n",
    "x_values = range(1, len(loss_evo) + 1)\n",
    "df_loss = pd.DataFrame({'Iteration': x_values, 'Loss': loss_evo, 'L_mc': loss_mc_evo, 'L_o': loss_o_evo})\n",
    "\n",
    "# Melt the DataFrame to have a single column for Loss Type\n",
    "df_loss = df_loss.melt(['Iteration'], ['Loss', 'L_mc', 'L_o'], 'Loss Type', 'Loss Value')\n",
    "\n",
    "# Plot the lines using Plotly Express\n",
    "fig = px.line(df_loss, x='Iteration', y='Loss Value', color='Loss Type',width=700).show()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(16,5))\n",
    "plotting_params = {'edge_color':'darkgray', 'edge_width':1.5,'vertex_color':'black', 'vertex_size':50}\n",
    "G.plotting.update(plotting_params)\n",
    "G.plot_signal(label, ax=ax[0], plot_name='')\n",
    "\n",
    "ax[0].collections[0].set_cmap(label_cmap)  # Modify the colormap of the plotted data\n",
    "ax[0].axis('off')\n",
    "\n",
    "print(S.argmax(dim=1).unique())\n",
    "G.plot_signal(np.array(S.argmax(dim=1)), ax=ax[1], plot_name='')\n",
    "ax[1].collections[0].set_cmap('viridis')\n",
    "ax[1].axis('off')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7,5))\n",
    "G.plot_signal(S.softmax(dim=-1).detach().numpy().max(axis=1), ax=ax, plot_name='')\n",
    "ax.collections[0].set_cmap('viridis')\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes['pred'] = np.array(S.argmax(dim=1))\n",
    "nodes['score'] = S.softmax(dim=-1).detach().numpy().max(axis=1)\n",
    "nodes['anomaly'] = df_anomaly[['pid','anomaly']].groupby('pid').anomaly.max().values\n",
    "\n",
    "utils.visualize_map(nodes, color='pred', size=np.ones(nodes.pid.nunique()), size_max=10,\n",
    "                     hover_data=['cluster'], zoom=15, figsize=(700,700), colormap='viridis')\n",
    "utils.visualize_map(nodes, color='score', size=np.ones(nodes.pid.nunique()), size_max=10,\n",
    "                     hover_data=['cluster'], zoom=15, figsize=(700,700), colormap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_preds = nodes.query('anomaly!=0').groupby('anomaly')['pred'].apply(lambda x: x.mode()[0])\n",
    "most_common_preds\n",
    "\n",
    "nodes['new_pred'] = nodes['pred']\n",
    "nodes.loc[~nodes.pred.isin(most_common_preds.values),'new_pred'] = 0\n",
    "\n",
    "max_anomaly = nodes.groupby('new_pred')['anomaly'].transform('max')\n",
    "\n",
    "# Replace non-zero values of 'new_pred' with the maximum 'anomaly' for that 'new_pred'\n",
    "nodes.loc[nodes['new_pred'] != 0, 'new_pred'] = max_anomaly\n",
    "\n",
    "utils.visualize_map(nodes, color='new_pred', size=np.ones(nodes.pid.nunique()), size_max=10,\n",
    "                     hover_data=['anomaly'], zoom=15, figsize=(700,700), colormap='viridis')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_true=nodes.anomaly, y_pred=nodes.new_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes.query('cluster==12').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes.query('cluster==0').pred.value_counts().index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_anomaly[['pid','cluster','anomaly']].copy().groupby('pid', as_index=False).max()\n",
    "df_test.query('cluster==0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.plot_signal(kmeans_feats.detach().numpy()[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xm.std(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class TimeSeriesNet(nn.Module):\n",
    "#     def __init__(self, num_time_series, num_channels, kernel_size, hidden_dim):\n",
    "#         super(TimeSeriesNet, self).__init__()\n",
    "#         self.shared_conv_layer = nn.Conv1d(in_channels=num_channels, out_channels=hidden_dim, kernel_size=kernel_size)\n",
    "#         self.graph_conv = GraphConvolution(input_dim=hidden_dim * num_time_series, output_dim=hidden_dim)\n",
    "\n",
    "#     def forward(self, time_series_data):\n",
    "#         # Concatenate all time series along the batch dimension\n",
    "#         batched_time_series_data = torch.stack(time_series_data, dim=0)\n",
    "        \n",
    "#         # Apply shared 1D convolution along the time dimension for all time series\n",
    "#         conv_out = self.shared_conv_layer(batched_time_series_data)\n",
    "#         conv_out = F.relu(conv_out)  # Apply ReLU activation\n",
    "        \n",
    "#         # Reshape the convolutional output to have time series as channels\n",
    "#         conv_out = conv_out.permute(0, 2, 1)  # Swap time and channel dimensions\n",
    "#         conv_out = conv_out.view(conv_out.size(0), -1, conv_out.size(2))  # Reshape to (batch_size, num_time_series, sequence_length)\n",
    "        \n",
    "#         # Apply graph convolution along the channel dimension\n",
    "#         graph_conv_out = self.graph_conv(conv_out)\n",
    "#         return graph_conv_out\n",
    "\n",
    "# # Example usage\n",
    "# num_time_series = 3\n",
    "# num_channels = 1\n",
    "# kernel_size = 3\n",
    "# hidden_dim = 64\n",
    "# graph_conv_input_dim = hidden_dim * num_time_series  # Adjust based on the size of merged features\n",
    "# graph_conv_output_dim = 64  # Adjust based on your task\n",
    "# graph_conv = GraphConvolution(input_dim=graph_conv_input_dim, output_dim=graph_conv_output_dim)\n",
    "\n",
    "# # Initialize your time series data\n",
    "# time_series_data = [torch.randn(batch_size, num_channels, sequence_length) for _ in range(num_time_series)]\n",
    "\n",
    "# # Create and forward through the network\n",
    "# model = TimeSeriesNet(num_time_series=num_time_series, num_channels=num_channels, kernel_size=kernel_size, hidden_dim=hidden_dim)\n",
    "# output = model(time_series_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class ClusterTS(nn.Module):\n",
    "#     def __init__(self, num_time_series, num_channels, kernel_size, hidden_dim):\n",
    "#         super(ClusterTS, self).__init__()\n",
    "#         self.conv_layers = nn.ModuleList([\n",
    "#             nn.Conv1d(in_channels=num_channels, out_channels=hidden_dim, kernel_size=kernel_size)\n",
    "#             for _ in range(num_time_series)\n",
    "#         ])\n",
    "#         self.glu_layers = nn.ModuleList([\n",
    "#             nn.GLU(dim=1)  # Apply GLU along the channel dimension\n",
    "#             for _ in range(num_time_series)\n",
    "#         ])\n",
    "#         self.graph_conv = GraphConvolution(input_dim=2 * hidden_dim, output_dim=hidden_dim)  # Concatenated feature dimension is 2 * hidden_dim\n",
    "\n",
    "#     def forward(self, time_series_data):\n",
    "#         conv_outputs = []\n",
    "#         glu_outputs = []\n",
    "#         for i, ts_data in enumerate(time_series_data):\n",
    "#             conv_out = self.conv_layers[i](ts_data)\n",
    "#             conv_out = F.relu(conv_out)  # Apply ReLU after convolution\n",
    "#             conv_outputs.append(conv_out)\n",
    "\n",
    "#             glu_out = self.glu_layers[i](ts_data)\n",
    "#             glu_outputs.append(glu_out)\n",
    "#         merged_features = torch.cat([torch.cat(conv_outputs, dim=1), torch.cat(glu_outputs, dim=1)], dim=1)  # Concatenate convolutional and GLU outputs along the channel dimension\n",
    "#         graph_conv_out = self.graph_conv(merged_features)\n",
    "#         return graph_conv_out\n",
    "\n",
    "# # Example usage\n",
    "# num_time_series = 3\n",
    "# num_channels = 1\n",
    "# kernel_size = 3\n",
    "# hidden_dim = 64\n",
    "# graph_conv_input_dim = 2 * hidden_dim  # Concatenated feature dimension\n",
    "# graph_conv_output_dim = 64  # Adjust based on your task\n",
    "# graph_conv = GraphConvolution(input_dim=graph_conv_input_dim, output_dim=graph_conv_output_dim)\n",
    "\n",
    "# # Initialize your time series data\n",
    "# time_series_data = [torch.randn(batch_size, num_channels, sequence_length) for _ in range(num_time_series)]\n",
    "\n",
    "# # Create and forward through the network\n",
    "# model = ClusterTS(num_time_series=num_time_series, num_channels=num_channels, kernel_size=kernel_size, hidden_dim=hidden_dim)\n",
    "# output = model(time_series_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C-mean, C/500, cat(C), factor = 0.25, using W, no norm X, cluster=5  \n",
    "tensor([0.7832, 0.6285, 0.5781, 0.9582, 0.8451, 0.6135, 0.5953, 0.6806, 0.5118,\n",
    "        0.7671, 0.8329, 1.5210, 1.7983, 1.3789, 1.0746, 1.1554, 0.2643, 0.4089,\n",
    "        0.4150, 0.6700, 1.1099, 1.3029, 0.5401, 0.6389, 0.2358, 0.2266],\n",
    "\n",
    "tensor([3.7696e-01, 3.3365e-01, 3.4405e-01, 3.4341e-01, 1.1247e+00, 1.9260e+00,\n",
    "        3.7696e-01, 4.7058e-01, 1.2311e-01, 1.5041e-01, 1.4726e-01, 1.4382e+00,\n",
    "        2.4234e+00, 1.6105e+00, 1.5961e-01, 2.5519e-01, 4.4617e-01, 4.5403e-01,\n",
    "        3.5307e-01, 5.2215e-01, 6.8282e-01, 1.5623e+00, 3.8356e-01, 6.4139e-01,\n",
    "        1.6950e-03, 1.6287e-03]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.get_edge_list()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.tensor(G.W.toarray()).float() #Using W as a float() tensor\n",
    "edge_index, _ = dense_to_sparse(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index, _ = dense_to_sparse(torch.tensor(G.A.toarray()))\n",
    "edge_index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sensors-BoU2skHt-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
