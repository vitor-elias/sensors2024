{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import scipy as sp\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import pygsp\n",
    "import optuna\n",
    "import joblib\n",
    "import gc\n",
    "import argparse\n",
    "import os\n",
    "import matplotlib\n",
    "import pickle\n",
    "import glob\n",
    "\n",
    "from matplotlib.ticker import ScalarFormatter, StrMethodFormatter, FormatStrFormatter, FuncFormatter\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import mean_squared_error, confusion_matrix, auc, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from optuna.samplers import TPESampler, BruteForceSampler\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import Linear, Conv1d, LayerNorm, DataParallel\n",
    "from torch_geometric.nn import GCNConv, Sequential, GraphConv\n",
    "from torch_geometric.nn.dense import mincut_pool, dense_mincut_pool\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "from torch.nn.functional import glu\n",
    "\n",
    "from torch_geometric.nn.models import GraphUNet\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_networkx, grid\n",
    "\n",
    "import sensors.utils.fault_detection as fd\n",
    "import sensors.utils.analysis as ana\n",
    "import sensors.utils.utils as utils\n",
    "\n",
    "from importlib import reload\n",
    "ana = reload(ana)\n",
    "utils = reload(utils)\n",
    "\n",
    "\n",
    "from pyprojroot import here\n",
    "ROOT_DIR = str(here())\n",
    "data_dir = '~/data/interim/'\n",
    "\n",
    "matplotlib.rcParams.update({'font.size': 20})\n",
    "matplotlib.rcParams.update({'font.family': 'DejaVu Serif'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'df_StOlavs_D1L2B'\n",
    "df_orig = pd.read_parquet(data_dir + f'{dataset}.parq')\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "df, nodes = fd.treat_nodes(df_orig)\n",
    "G, nodes['subgraph'] = fd.NNGraph(nodes, radius=15, subgraphs=True)\n",
    "\n",
    "main_graph = nodes.subgraph.value_counts().index[0]\n",
    "nodes = nodes.query('subgraph==@main_graph')\n",
    "G = fd.NNGraph(nodes, radius=15)\n",
    "df = df[df.pid.isin(nodes.pid.unique())]\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "nodes['cluster'] = KMeans(n_clusters=20, n_init='auto').fit_predict(nodes[['northing','easting']])\n",
    "df.drop('cluster', axis=1, inplace=True)\n",
    "df = df.merge(nodes[['pid','cluster']], how='left', on='pid')\n",
    "df['strcluster'] = df.cluster.astype(str).values\n",
    "\n",
    "df_anomaly = df.copy()\n",
    "df_anomaly['anomaly'] = 0\n",
    "\n",
    "# Anomaly test - different anomalous clusters\n",
    "# Anomaly 1\n",
    "anomaly_sensor = (df_anomaly.cluster==11)\n",
    "anomaly_onset = (df_anomaly.timestamp>'Jul 2020')#&(df_anomaly.timestamp<'Jan 2021')\n",
    "anomaly_loc = anomaly_sensor&anomaly_onset\n",
    "\n",
    "df_anomaly.loc[anomaly_loc, 'smoothed'] += 5\n",
    "df_anomaly.loc[anomaly_loc, 'anomaly'] = 1\n",
    "\n",
    "# # Anomaly 2\n",
    "# anomaly_sensor = (df_anomaly.cluster==12)\n",
    "# anomaly_onset = (df_anomaly.timestamp>'Jul 2020')\n",
    "# anomaly_loc = anomaly_sensor&anomaly_onset\n",
    "\n",
    "# df_anomaly.loc[anomaly_loc, 'smoothed'] += 5\n",
    "# df_anomaly.loc[anomaly_loc, 'anomaly'] = 2\n",
    "\n",
    "# # Anomaly 3\n",
    "# anomaly_sensor = (df_anomaly.cluster==19)\n",
    "# anomaly_onset = (df_anomaly.timestamp>'Jul 2020')\n",
    "# anomaly_loc = anomaly_sensor&anomaly_onset\n",
    "\n",
    "# df_anomaly.loc[anomaly_loc, 'smoothed'] -= 5\n",
    "# df_anomaly.loc[anomaly_loc, 'anomaly'] = 3\n",
    "\n",
    "X = df_anomaly.pivot(index='pid', columns='timestamp', values='smoothed').values\n",
    "X = torch.tensor(X)\n",
    "\n",
    "label = df_anomaly.pivot(index='pid', columns='timestamp', values='anomaly').values.max(axis=1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "G.plot_signal(label, ax=ax, plot_name='')\n",
    "\n",
    "# label_cmap = ListedColormap(['blue','goldenrod', 'sienna','aquamarine'])\n",
    "label_cmap = ListedColormap(plt.cm.viridis(np.linspace(0,1,df_anomaly.anomaly.nunique())))\n",
    "\n",
    "ax.collections[0].set_cmap(label_cmap)  # Modify the colormap of the plotted data\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of clusters for each feature\n",
    "def kmeans_features(data, num_clusters):\n",
    "\n",
    "    def cluster_kmeans(tensor, k):\n",
    "        kmeans = KMeans(n_clusters=k, n_init=1)\n",
    "        kmeans.fit(tensor)\n",
    "        return kmeans.labels_\n",
    "\n",
    "    kmeans_features = []\n",
    "    # Perform clustering for each number of clusters\n",
    "    for k in num_clusters:\n",
    "        # Perform K-means clustering\n",
    "        cluster_labels = cluster_kmeans(data, k)\n",
    "        kmeans_features.append(cluster_labels)\n",
    "\n",
    "    return torch.tensor(np.array(kmeans_features).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusterTS(nn.Module):\n",
    "    def __init__(self,\n",
    "                 conv1d_n_feats, conv1d_kernel_size, conv1d_stride,\n",
    "                 graphconv_n_feats,\n",
    "                 n_timestamps,\n",
    "                 n_clusters,\n",
    "                 n_extra_feats):\n",
    "        \n",
    "        super(ClusterTS, self).__init__()\n",
    "        self.conv1d = nn.Conv1d(in_channels=1, out_channels=conv1d_n_feats,\n",
    "                                kernel_size=conv1d_kernel_size, stride=conv1d_stride)\n",
    "        \n",
    "        self.L_in = n_timestamps\n",
    "        self.L_out = math.floor((self.L_in - conv1d_kernel_size)/conv1d_stride + 1)\n",
    "\n",
    "        self.conv1d_out = conv1d_n_feats*self.L_out\n",
    "        \n",
    "        mlp_in = self.conv1d_out + n_extra_feats\n",
    "        self.mcp_mlp = Linear(mlp_in, n_clusters)\n",
    "    \n",
    "    def forward(self, X, A, extra_feats=None):\n",
    "\n",
    "        #HP\n",
    "        factor_coords = 0.5\n",
    "\n",
    "        # Data\n",
    "        X = X.float()\n",
    "        norm_X = LayerNorm(X.shape, elementwise_affine=False)\n",
    "        X = norm_X(X)\n",
    "\n",
    "        X = X.unsqueeze(1) # adjusting shape for conv1d\n",
    "        X = self.conv1d(X)\n",
    "\n",
    "        X = X.reshape((X.shape[0],-1)) #\n",
    "\n",
    "        if extra_feats is not None:\n",
    "            norm_f = LayerNorm(extra_feats.shape, elementwise_affine=False)\n",
    "            extra_feats = factor_coords*norm_f(extra_feats)\n",
    "            X = torch.cat((X,extra_feats),dim=1)\n",
    "\n",
    "        S = self.mcp_mlp(X)\n",
    "\n",
    "        _, _, loss_mc, loss_o = dense_mincut_pool(X, A, S)\n",
    "\n",
    "        # return torch.softmax(S, dim=-1), loss_mc, loss_o\n",
    "        return S, loss_mc, loss_o, X, C\n",
    "    \n",
    "\n",
    "# torch.random.manual_seed(0)\n",
    "\n",
    "conv1d_n_feats = 3\n",
    "conv1d_kernel_size = 60\n",
    "conv1d_stride = 30\n",
    "\n",
    "graphconv_n_feats = 30\n",
    "\n",
    "n_nodes = X.shape[0]\n",
    "n_timestamps = X.shape[1]\n",
    "\n",
    "n_clusters = 20\n",
    "factor = 0.25\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "X = X.to(device)\n",
    "\n",
    "# Node coordinates\n",
    "C = torch.tensor(G.coords)\n",
    "A = torch.tensor(G.W.toarray()).float() #Using W as a float() tensor\n",
    "A = A.to(device)\n",
    "\n",
    "np.random.seed(2)\n",
    "\n",
    "num_clusters_per_feature = [5, 6]\n",
    "kmeans_feats = kmeans_features(C, num_clusters_per_feature).to(device).float()\n",
    "n_extra_feats = kmeans_feats.shape[1]\n",
    "\n",
    "# kmeans_feats = None\n",
    "# n_extra_feats = 0\n",
    "\n",
    "model = ClusterTS(conv1d_n_feats, conv1d_kernel_size, conv1d_stride, graphconv_n_feats,\n",
    "                  n_timestamps, n_clusters, n_extra_feats)\n",
    "# model.conv1d = DataParallel(model.conv1d)\n",
    "# model.mcp_mlp = DataParallel(model.mcp_mlp)\n",
    "model = model.to(device)\n",
    "\n",
    "print(f'Clusters: {n_clusters}')\n",
    "print(f'Factor: {factor}')\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def train(N_epochs):\n",
    " \n",
    "    loss_evo = []\n",
    "    loss_mc_evo = []\n",
    "    loss_o_evo = []\n",
    "\n",
    "    model.train()\n",
    "    for epoch in tqdm(range(N_epochs)):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        S, loss_mc, loss_o, Xm, Cm = model(X, A, kmeans_feats)\n",
    "        loss = loss_mc + factor*loss_o\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_evo.append(loss.item())\n",
    "        loss_mc_evo.append(loss_mc.item())\n",
    "        loss_o_evo.append(loss_o.item())\n",
    "\n",
    "    return loss_evo, loss_mc_evo, loss_o_evo, S\n",
    "\n",
    "\n",
    "loss_evo, loss_mc_evo, loss_o_evo, S = train(10000)\n",
    "\n",
    "# model.eval()\n",
    "# S, _, _, Xm, Cm = model(X, A, kmeans_feats)\n",
    "S = S.cpu()\n",
    "\n",
    "\n",
    "print(model)\n",
    "\n",
    "x_values = range(1, len(loss_evo) + 1)\n",
    "df_loss = pd.DataFrame({'Iteration': x_values, 'Loss': loss_evo, 'L_mc': loss_mc_evo, 'L_o': loss_o_evo})\n",
    "\n",
    "# Melt the DataFrame to have a single column for Loss Type\n",
    "df_loss = df_loss.melt(['Iteration'], ['Loss', 'L_mc', 'L_o'], 'Loss Type', 'Loss Value')\n",
    "\n",
    "# Plot the lines using Plotly Express\n",
    "fig = px.line(df_loss, x='Iteration', y='Loss Value', color='Loss Type',width=700).show()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(16,5))\n",
    "plotting_params = {'edge_color':'darkgray', 'edge_width':1.5,'vertex_color':'black', 'vertex_size':50}\n",
    "G.plotting.update(plotting_params)\n",
    "G.plot_signal(label, ax=ax[0], plot_name='')\n",
    "\n",
    "ax[0].collections[0].set_cmap(label_cmap)  # Modify the colormap of the plotted data\n",
    "ax[0].axis('off')\n",
    "\n",
    "print(S.argmax(dim=1).unique())\n",
    "G.plot_signal(np.array(S.argmax(dim=1)), ax=ax[1], plot_name='')\n",
    "ax[1].collections[0].set_cmap('viridis')\n",
    "ax[1].axis('off')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7,5))\n",
    "G.plot_signal(S.softmax(dim=-1).detach().numpy().max(axis=1), ax=ax, plot_name='')\n",
    "ax.collections[0].set_cmap('viridis')\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes['pred'] = np.array(S.argmax(dim=1))\n",
    "nodes['score'] = S.softmax(dim=-1).detach().numpy().max(axis=1)\n",
    "nodes['anomaly'] = df_anomaly[['pid','anomaly']].groupby('pid').anomaly.max().values\n",
    "\n",
    "most_common_preds = nodes.query('anomaly!=0').groupby('anomaly')['pred'].apply(lambda x: x.mode()[0])\n",
    "\n",
    "nodes['new_pred'] = nodes['pred']\n",
    "nodes.loc[~nodes.pred.isin(most_common_preds.values),'new_pred'] = -1\n",
    "\n",
    "max_anomaly = nodes.groupby('new_pred')['anomaly'].transform('max')\n",
    "nodes.loc[nodes['new_pred'] != -1, 'new_pred'] = max_anomaly\n",
    "nodes.loc[nodes['new_pred'] == -1, 'new_pred'] = 0\n",
    "\n",
    "\n",
    "utils.visualize_map(nodes, color='pred', size=np.ones(nodes.pid.nunique()), size_max=10,\n",
    "                     hover_data=['cluster'], zoom=15, figsize=(600,600), colormap='viridis')\n",
    "utils.visualize_map(nodes, color='score', size=np.ones(nodes.pid.nunique()), size_max=10,\n",
    "                     hover_data=['cluster'], zoom=15, figsize=(600,600), colormap='viridis')\n",
    "utils.visualize_map(nodes, color='new_pred', size=np.ones(nodes.pid.nunique()), size_max=10,\n",
    "                     hover_data=['anomaly'], zoom=15, figsize=(600,600), colormap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_true=nodes.anomaly, y_pred=nodes.new_pred, average='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anomaly.anomaly.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sensors-BoU2skHt-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
