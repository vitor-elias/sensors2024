{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import scipy as sp\n",
    "import osmnx as ox\n",
    "import geopandas as gpd\n",
    "import tensorflow as tf\n",
    "import ruptures as rpt\n",
    "import contextily as cx\n",
    "\n",
    "import optuna\n",
    "import torch\n",
    "import pygsp\n",
    "\n",
    "from matplotlib.ticker import ScalarFormatter, StrMethodFormatter, FormatStrFormatter, FuncFormatter\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "from collections import Counter\n",
    "\n",
    "from tslearn.metrics import dtw\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance, TimeSeriesScalerMinMax\n",
    "from tslearn.shapelets import LearningShapelets, grabocka_params_to_shapelet_size_dict\n",
    "from sklearn.metrics import mean_squared_error, confusion_matrix, auc\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sktime.transformations.panel.shapelet_transform import ShapeletTransform\n",
    "from sktime.classification.shapelet_based import ShapeletTransformClassifier\n",
    "\n",
    "from importlib import reload\n",
    "from pyprojroot import here\n",
    "ROOT_DIR = str(here())\n",
    "\n",
    "import dario.models.mismatch_analysis as mma\n",
    "from dario.models.maxdiv.maxdiv import maxdiv\n",
    "mma = reload(mma)\n",
    "\n",
    "insar_path = ROOT_DIR + \"/data/raw/insar/\"\n",
    "\n",
    "\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "plt.rcParams.update({'font.family': 'Times New Roman'})\n",
    "\n",
    "def compute_metric(df_test, cut=2, radius=15):\n",
    "\n",
    "    df_metrics = []\n",
    "    for cluster in sorted(df_test.cluster.unique()):\n",
    "\n",
    "        df, nodes = mma.treat_nodes(df_test.query('cluster==@cluster'))\n",
    "        G, nodes['subgraph'] = mma.NNGraph(nodes, radius=radius, subgraphs=True)\n",
    "\n",
    "        df_metrics_cluster = []\n",
    "        for sub_index in sorted(nodes.subgraph.unique())[1:]:\n",
    "\n",
    "            subnodes = nodes.query('subgraph==@sub_index').copy()\n",
    "            subdf = df[df.pid.isin(subnodes.pid)].copy()\n",
    "\n",
    "            G = mma.NNGraph(subnodes, radius=radius)\n",
    "\n",
    "            w, V = np.linalg.eigh(G.L.toarray())\n",
    "            wh = np.ones(G.N)\n",
    "            wh[w<cut] = 0\n",
    "            Hh = V @ np.diag(wh) @ V.T\n",
    "\n",
    "            smoothed = subdf[['pid', 'timestamp', 'smoothed' ]].pivot(index='pid', columns='timestamp')\n",
    "\n",
    "            subdf['hf'] = np.abs((Hh @ smoothed.values).reshape((-1,), order='C'))\n",
    "\n",
    "            df_metrics_cluster.append(subdf)\n",
    "\n",
    "        df_metrics_cluster = pd.concat(df_metrics_cluster)\n",
    "        df_metrics.append(df_metrics_cluster)\n",
    "\n",
    "    df_metrics = pd.concat(df_metrics)\n",
    "    return df_metrics\n",
    "\n",
    "def detection(df_metrics, column_name='wse', threshold_min=1000, threshold_max=np.inf, selector='group',\n",
    "              detection_param='detection_sum', detection_param_threshold=None):\n",
    "    # df_relevant contains data from nodes that, at some point, have lower<=wse<=upper, and their neighbors.\n",
    "    # nodes are put into groups if they are close to each other.\n",
    "\n",
    "    if detection_param_threshold is None:\n",
    "        detection_param_threshold = df_metrics.timestamp.nunique()//2\n",
    "\n",
    "    df_relevant = mma.relevant_neighborhood(df_metrics, column_name=column_name,\n",
    "                                            lower=threshold_min, upper=threshold_max,\n",
    "                                            only_relevant=True, return_df=True, plot=False, filter_dates=False)\n",
    "\n",
    "    # Treating disconnected nodes as individual groups. Assining new group values to these nodes\n",
    "    new_group_values = df_relevant.query('group==0').pid.factorize()[0] + df_relevant.group.max()+1\n",
    "    df_relevant.loc[df_relevant.group==0, 'group'] = new_group_values\n",
    "\n",
    "    # Creates a detection column that is True for the timestamps when the metric is inside the thresholds\n",
    "    # These are \"partial detections\". We will consider anomaly depending on how many partial detections are found\n",
    "    df_relevant['detection'] = (df_relevant[column_name]>=threshold_min) & (df_relevant[column_name]<=threshold_max)\n",
    "\n",
    "    # Counting partial detections (total, and consecutive occurances)\n",
    "    df_detection = df_relevant.groupby('pid').agg({column_name:['max','mean'],\n",
    "                                                    'detection':['sum',mma.consecutive_ones],\n",
    "                                                    'group':'mean'}).reset_index()\n",
    "    df_detection.columns = [f\"{level1}_{level2}\" if level2 else level1 for level1, level2 in df_detection.columns]\n",
    "    df_detection.rename({'group_mean':'group'}, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    # Querying for the cases where \"actual detection\" happens\n",
    "    # Selected can return pids or groups, depending on the value of \"selector\"\n",
    "    query = f'{detection_param}>{detection_param_threshold}'\n",
    "    selected = df_detection.query(query)[selector].unique()\n",
    "\n",
    "    return df_relevant, selected\n",
    "\n",
    "def detectionv2(df_metrics, column_name='wse', quantile=0.995, detect_ratio = 0.5):\n",
    "\n",
    "    \n",
    "    detection_param_threshold = np.ceil(detect_ratio*df_metrics.timestamp.nunique())\n",
    "\n",
    "    selected_pixels=[]\n",
    "\n",
    "    for ts in df_metrics.timestamp.unique():\n",
    "        df = df_metrics[df_metrics.timestamp==ts].copy()\n",
    "        th = df_metrics[column_name].quantile(quantile)\n",
    "\n",
    "        selected_pixels.append(df[df[column_name] >= th].pid.unique())\n",
    "\n",
    "    flat_list = [item for sublist in selected_pixels for item in sublist]\n",
    "    id_counts = Counter(flat_list)\n",
    "    df_anomaly_count = pd.DataFrame({'pid':id_counts.keys(), 'count':id_counts.values()})\n",
    "\n",
    "    faulty_pixels = df_anomaly_count.query('count>@detection_param_threshold').pid.unique()\n",
    "\n",
    "    return faulty_pixels\n",
    "    \n",
    "\n",
    "def bounds_around_id(id, max_value, min_value=0, samples_before=30, samples_after=60):\n",
    "    # Calculate the lower and upper bounds of the range\n",
    "    # returns also:\n",
    "    # case 0 if there are enough samples before and after\n",
    "    # case 1 if not enough samples after\n",
    "    # case 2 if not enough samples before    \n",
    "    range_size= samples_before + samples_after\n",
    "    lower_bound = id - samples_before\n",
    "    upper_bound = id + samples_after\n",
    "\n",
    "    case = 0\n",
    "    \n",
    "    # Adjust the range if it is too close to the edges\n",
    "    if lower_bound < min_value:\n",
    "        lower_bound = min_value\n",
    "        upper_bound = min(range_size, max_value)\n",
    "        case = 2\n",
    "    elif upper_bound > max_value:\n",
    "        upper_bound = max_value\n",
    "        lower_bound = max(max_value - range_size, min_value)\n",
    "        case = 1\n",
    "    \n",
    "    return lower_bound, upper_bound, case\n",
    "\n",
    "def dtwkmeans(df, cluster_by='smoothed', n_clusters=6, n_init=5, savefile=None):\n",
    "\n",
    "    data_ts = df[cluster_by].values.reshape((-1, df.pid.value_counts()[0],1))\n",
    "    model = TimeSeriesKMeans(n_clusters=n_clusters, metric=\"dtw\", n_init=n_init)\n",
    "    y_pred = model.fit_predict(data_ts)\n",
    "\n",
    "    # Plotting\n",
    "    nrows = int(np.ceil(n_clusters/3))\n",
    "    ncols = 3\n",
    "    ratios = [3]\n",
    "    ratios.extend([0.4,3]*(nrows-1))\n",
    "\n",
    "    if n_clusters == 4:\n",
    "        ncols = 2\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=2*nrows -1, ncols=ncols, figsize=(30,12), gridspec_kw={'height_ratios': ratios})\n",
    "\n",
    "    for cluster in range(n_clusters):\n",
    "\n",
    "        if df.timestamp.nunique()==data_ts.shape[1]:\n",
    "            timestamp_list = df.timestamp.unique()\n",
    "        else:\n",
    "            timestamp_list = range(data_ts.shape[1])\n",
    "            \n",
    "        for timeseries in data_ts[y_pred == cluster]:\n",
    "            ax[(cluster//ncols)*2,cluster%ncols].plot(timestamp_list, timeseries.ravel(), alpha=0.1)\n",
    "        \n",
    "        ax[(cluster//ncols)*2,cluster%ncols].text(0.05, 0.95, \n",
    "                                            str(sum(y_pred==cluster)),\n",
    "                                            transform=ax[(cluster//ncols)*2,cluster%ncols].transAxes)\n",
    "\n",
    "    for i in range(nrows):\n",
    "        if ~i%2:\n",
    "            ax[i,0].set_ylabel('Displacement')\n",
    "            ax[i,0].set_ylabel('Displacement')\n",
    "        else:\n",
    "            for j in range(ncols):\n",
    "                ax[i,j].set_visible(False)\n",
    "\n",
    "    ax[-1,1].set_xlabel('Timestamp')\n",
    "    if ncols==2:\n",
    "        ax[-1,0].set_xlabel('Timestamp')\n",
    "\n",
    "    fig.suptitle('TimeSeriesKmeans')\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(hspace=0.05, wspace=0.05)\n",
    "    if savefile is not None:\n",
    "        plt.savefig(ROOT_DIR+savefile)\n",
    "    plt.show()\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "def plot_dtwkmeans(df, cluster_by='smoothed', savefile=None):\n",
    "\n",
    "    data_ts = df[cluster_by].values.reshape((-1, df.pid.value_counts()[0],1))\n",
    "    y_pred = df.drop_duplicates('pid')['grad_cluster'].values\n",
    "\n",
    "    n_clusters = df.grad_cluster.nunique()\n",
    "\n",
    "    # Plotting\n",
    "    nrows = int(np.ceil(n_clusters/3))\n",
    "    ncols = 3\n",
    "    ratios = [3]\n",
    "    ratios.extend([0.4,3]*(nrows-1))\n",
    "\n",
    "    if n_clusters == 4:\n",
    "        ncols = 2\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=2*nrows -1, ncols=ncols, figsize=(30,12), gridspec_kw={'height_ratios': ratios})\n",
    "\n",
    "    for cluster in range(n_clusters):\n",
    "\n",
    "        if df.timestamp.nunique()==data_ts.shape[1]:\n",
    "            timestamp_list = df.timestamp.unique()\n",
    "        else:\n",
    "            timestamp_list = range(data_ts.shape[1])\n",
    "            \n",
    "        for timeseries in data_ts[y_pred == cluster]:\n",
    "            ax[(cluster//ncols)*2,cluster%ncols].plot(timestamp_list, timeseries.ravel(), alpha=0.1)\n",
    "        \n",
    "        ax[(cluster//ncols)*2,cluster%ncols].text(0.05, 0.95, \n",
    "                                            str(sum(y_pred==cluster)),\n",
    "                                            transform=ax[(cluster//ncols)*2,cluster%ncols].transAxes)\n",
    "\n",
    "    for i in range(nrows):\n",
    "        if ~i%2:\n",
    "            ax[i,0].set_ylabel('Displacement')\n",
    "            ax[i,0].set_ylabel('Displacement')\n",
    "        else:\n",
    "            for j in range(ncols):\n",
    "                ax[i,j].set_visible(False)\n",
    "\n",
    "    ax[-1,1].set_xlabel('Timestamp')\n",
    "    if ncols==2:\n",
    "        ax[-1,0].set_xlabel('Timestamp')\n",
    "\n",
    "    fig.suptitle('TimeSeriesKmeans')\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(hspace=0.05, wspace=0.05)\n",
    "    if savefile is not None:\n",
    "        plt.savefig(ROOT_DIR+savefile)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'df_Porsgrunn_A1L2B'\n",
    "df_orig = pd.read_parquet(ROOT_DIR+f\"/data/interim/{dataset}.parq\")\n",
    "df_orig\n",
    "print(df_orig.pid.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mma.relevant_neighborhood(df_metrics, column_name='hf', lower=60, zoom=11, range_meters=15,\n",
    "                          only_relevant=False, filter_dates=False, by_max=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing new hf for IGARSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "cut = 2 # frequency cut\n",
    "radius = 20 # radius for constructing NNGraph\n",
    "\n",
    "df = df_orig.copy()\n",
    "\n",
    "df_metrics = compute_metric(df, cut, radius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig.timestamp.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = detectionv2(df_metrics, column_name='hf', quantile=0.995, detect_ratio=0.6)\n",
    "len(selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mma.plot_selected_map(df_metrics, column_name='hf', size='hf', selected_pixels=selected, only_relevant=False,\n",
    "                     figsize=(1600,900))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANOMALY FILTERING\n",
    "\n",
    "# Doing two rounds of high-frequency filtering\n",
    "# First round filters out the very-high frequency and avoids pixels with indirect high-frequency\n",
    "# Second round filters out pixels that still have high-frequency that is not caused by the very-high frequency pixels\n",
    "\n",
    "th1 = 70 # round 1 threshold\n",
    "th2 = 40 # round 2 threshold\n",
    "th_hits = 10 # number of timestamps hitting threshold to assign anomaly\n",
    "\n",
    "cut = 2 # frequency cut\n",
    "radius = 20 # radius for constructing NNGraph\n",
    "\n",
    "df = df_orig.copy()\n",
    "\n",
    "# round 1\n",
    "df_metrics = compute_metric(df, cut, radius)\n",
    "df_detection, selected = detection(df_metrics, column_name='hf', threshold_min=th1, selector='pid', \n",
    "                                   detection_param='detection_sum', detection_param_threshold=th_hits)\n",
    "df = df[~df.pid.isin(selected)]\n",
    "\n",
    "# # round 2\n",
    "# df_metrics = compute_metric(df, cut, radius)\n",
    "# df_detection, selected = detection(df_metrics, column_name='hf', threshold_min=th2, selector='pid',\n",
    "#                                    detection_param='detection_sum', detection_param_threshold=th_hits)\n",
    "# df = df[~df.pid.isin(selected)]\n",
    "\n",
    "print(df.pid.nunique())\n",
    "\n",
    "# Since metrics are not computed for lone pixels, merge into the original dataframe to include those\n",
    "# Lone pixels get value 0 in the metric\n",
    "df = df.merge(df_metrics[['hf']], how='left',left_index=True, right_index=True)\n",
    "df = df.fillna(0)\n",
    "# df.to_parquet(ROOT_DIR+f\"/data/interim/{dataset}_filtered.parq\")\n",
    "df_filtered = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = sorted(df.timestamp.unique())\n",
    "midtime = len(times)//2\n",
    "plt.rcParams.update({'font.size': 40})\n",
    "plt.rcParams.update({'font.family': 'Times New Roman'})\n",
    "fig, ax = plt.subplots(ncols=2, nrows=2, figsize=(26,12))\n",
    "ax[0,0].hist(df_metrics.hf.values, log=True, density=True, bins=100)\n",
    "ax[0,0].set_title('All data')\n",
    "\n",
    "ax[0,1].hist(df_metrics.query('timestamp==@times[0]').hf.values, log=True, density=True, bins=100)\n",
    "ax[0,1].set_title('First timestamp')\n",
    "\n",
    "ax[1,0].hist(df_metrics.query('timestamp==@times[@midtime]').hf.values, log=True, density=True, bins=100)\n",
    "ax[1,0].set_title('Central timestamp')\n",
    "\n",
    "ax[1,1].hist(df_metrics.query('timestamp==@times[-1]').hf.values, log=True, density=True, bins=100)\n",
    "ax[1,1].set_title('Last timestamp')\n",
    "\n",
    "for axis in ax.flatten():\n",
    "    axis.set_xlabel('Fault score')\n",
    "    axis.tick_params(axis='both', which='both', labelsize=18)\n",
    "    axis.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(ROOT_DIR+\"/models/outputs/figs/ReportESA/score_distribution.png\", transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics.query('timestamp==@times[-1]').hf.quantile(0.995)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mma.plot_selected_map(df_metrics, column_name='hf', size='hf', selected_pixels=selected, figsize=(1600,900))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "df_test = df_orig[df_orig.pid.isin(selected)].drop_duplicates('pid')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,9))\n",
    "# colors = df_test.mean_velocity.astype('category').cat.codes\n",
    "ax.scatter(df_test.longitude, df_test.latitude, s=1, cmap='Greys')\n",
    "\n",
    "ax.xaxis.set_visible(False)\n",
    "ax.yaxis.set_visible(False)\n",
    "# Graph.plot(ax=ax, plot_name='')\n",
    "cx.add_basemap(ax, crs='epsg:4326', source=cx.providers.OpenStreetMap.Mapnik)\n",
    "plt.savefig(ROOT_DIR+\"/models/outputs/figs/ReportESA/Porsgrunn_detections.png\", transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_single = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANOMALY FILTERING\n",
    "\n",
    "# Doing two rounds of high-frequency filtering\n",
    "# First round filters out the very-high frequency and avoids pixels with indirect high-frequency\n",
    "# Second round filters out pixels that still have high-frequency that is not caused by the very-high frequency pixels\n",
    "\n",
    "th1 = 70 # round 1 threshold\n",
    "th2 = 40 # round 2 threshold\n",
    "th_hits = 10 # number of timestamps hitting threshold to assign anomaly\n",
    "\n",
    "cut = 2 # frequency cut\n",
    "radius = 20 # radius for constructing NNGraph\n",
    "\n",
    "df = df_orig.copy()\n",
    "\n",
    "# round 1\n",
    "df_metrics = compute_metric(df, cut, radius)\n",
    "df_detection, selected = detection(df_metrics, column_name='hf', threshold_min=th1, selector='pid', \n",
    "                                   detection_param='detection_sum', detection_param_threshold=th_hits)\n",
    "df = df[~df.pid.isin(selected)]\n",
    "\n",
    "# round 2\n",
    "df_metrics = compute_metric(df, cut, radius)\n",
    "df_detection, selected = detection(df_metrics, column_name='hf', threshold_min=th2, selector='pid',\n",
    "                                   detection_param='detection_sum', detection_param_threshold=th_hits)\n",
    "df = df[~df.pid.isin(selected)]\n",
    "\n",
    "print(df.pid.nunique())\n",
    "\n",
    "# Since metrics are not computed for lone pixels, merge into the original dataframe to include those\n",
    "# Lone pixels get value 0 in the metric\n",
    "df = df.merge(df_metrics[['hf']], how='left',left_index=True, right_index=True)\n",
    "df = df.fillna(0)\n",
    "# df.to_parquet(ROOT_DIR+f\"/data/interim/{dataset}_filtered.parq\")\n",
    "df_filtered = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_double = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.pid.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_single = set(df_orig.pid.unique())-set(df_single.pid.unique())\n",
    "detect_double = set(df_orig.pid.unique())-set(df_double.pid.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersect = detect_single.intersection(detect_double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(detect_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(detect_double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(intersect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig.pid.nunique()-df_single.pid.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mma.relevant_neighborhood(df_metrics, column_name='hf', lower=30, zoom=11, range_meters=15,\n",
    "                          only_relevant=False, filter_dates=False, by_max=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(ROOT_DIR+f\"/data/interim/{dataset}_filtered.parq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(ROOT_DIR+f\"/data/interim/{dataset}_filtered.parq\")\n",
    "df_filtered = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_filtered.copy()\n",
    "min_relevant_smoothed = 30\n",
    "relevant_pids = (df[['pid','smoothed']]\n",
    "                 .groupby('pid',as_index=False)\n",
    "                 .agg(lambda x: np.ptp(x))\n",
    "                 .query('smoothed>@min_relevant_smoothed')\n",
    "                 .pid.unique()\n",
    ")\n",
    "print(len(relevant_pids))\n",
    "\n",
    "df = df[df.pid.isin(relevant_pids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 6\n",
    "\n",
    "data_ts = df.smoothed.values.reshape((-1, df.timestamp.nunique(),1))\n",
    "\n",
    "model = TimeSeriesKMeans(n_clusters=n_clusters, metric=\"dtw\", max_iter=20)\n",
    "y_pred = model.fit_predict(data_ts)\n",
    "\n",
    "df['tskmeans_6'] = np.repeat(y_pred, df.timestamp.nunique())\n",
    "# df.to_parquet(ROOT_DIR+f\"/data/interim/{dataset}_tskmeans.parq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(ROOT_DIR+f\"/data/interim/{dataset}_tskmeans.parq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = df.tskmeans_6.nunique()\n",
    "data_ts = df.smoothed.values.reshape((-1, df.timestamp.nunique(),1))\n",
    "y_pred = df.groupby('pid').tskmeans_6.min().values\n",
    "fig, ax = plt.subplots(nrows=3, ncols=3, figsize=(30,12), gridspec_kw={'height_ratios': [3,0.4,3]})\n",
    "\n",
    "for cluster in range(n_clusters):\n",
    "\n",
    "    timestamp_list = df.timestamp.unique()\n",
    "    for timeseries in data_ts[y_pred == cluster]:\n",
    "        ax[(cluster//3)*2,cluster%3].plot(timestamp_list, timeseries.ravel(), alpha=0.1)\n",
    "    \n",
    "    ax[(cluster//3)*2,cluster%3].text(0.05, 0.95, \n",
    "                                        str(sum(y_pred==cluster)),\n",
    "                                        transform=ax[(cluster//3)*2,cluster%3].transAxes)\n",
    "\n",
    "ax[0,0].set_ylabel('Displacement')\n",
    "ax[2,0].set_ylabel('Displacement')\n",
    "ax[2,1].set_xlabel('Timestamp')\n",
    "ax[1,0].set_visible(False)\n",
    "ax[1,1].set_visible(False)\n",
    "ax[1,2].set_visible(False)\n",
    "\n",
    "fig.suptitle('TimeSeriesKmeans')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(hspace=0.05, wspace=0.05)\n",
    "# plt.savefig(ROOT_DIR+'/models/outputs/clustering_TimeSeriesKmeans.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 6\n",
    "\n",
    "data_ts = df.query('tskmeans_6==0').smoothed.values.reshape((-1, df.timestamp.nunique(),1))\n",
    "\n",
    "model = TimeSeriesKMeans(n_clusters=n_clusters, metric=\"dtw\", max_iter=20)\n",
    "y_pred = model.fit_predict(data_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=3, ncols=3, figsize=(30,12), gridspec_kw={'height_ratios': [3,0.4,3]})\n",
    "\n",
    "for cluster in range(n_clusters):\n",
    "\n",
    "    timestamp_list = df.timestamp.unique()\n",
    "    for timeseries in data_ts[y_pred == cluster]:\n",
    "        ax[(cluster//3)*2,cluster%3].plot(timestamp_list, timeseries.ravel(), alpha=0.1)\n",
    "    \n",
    "    ax[(cluster//3)*2,cluster%3].text(0.05, 0.95, \n",
    "                                        str(sum(y_pred==cluster)),\n",
    "                                        transform=ax[(cluster//3)*2,cluster%3].transAxes)\n",
    "\n",
    "ax[0,0].set_ylabel('Displacement')\n",
    "ax[2,0].set_ylabel('Displacement')\n",
    "ax[2,1].set_xlabel('Timestamp')\n",
    "ax[1,0].set_visible(False)\n",
    "ax[1,1].set_visible(False)\n",
    "ax[1,2].set_visible(False)\n",
    "\n",
    "fig.suptitle('TimeSeriesKmeans')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(hspace=0.05, wspace=0.05)\n",
    "# plt.savefig(ROOT_DIR+'/models/outputs/clustering_TimeSeriesKmeans.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_on_gradmax(group):\n",
    "    max_grad_abs_idx = group['grad_abs'].idxmax()  # Index of max 'grad_abs'\n",
    "    max_smoothed = group.at[max_grad_abs_idx, 'smoothed']  # Value of 'smoothed' at max 'grad_abs'\n",
    "    group['centered_on_gradmax'] -= max_smoothed  # Normalize 'smoothed'\n",
    "    return group\n",
    "\n",
    "\n",
    "def get_df_onset(df, threshold, clustering_length, prediction_range=30, pelt=None):\n",
    "    # Receives a partial df for a single pixel\n",
    "\n",
    "    if pelt is None:\n",
    "        pelt = rpt.Pelt(model='rbf', min_size=60)\n",
    "\n",
    "    ts = df.smoothed.values\n",
    "    gs = df.grad_abs.values\n",
    "\n",
    "    change = np.array(pelt.fit_predict(ts, pen=1))\n",
    "    change_tuples = [(0, change[0])]+[(change[i]+1,change[i+1]) for i in range(len(change)-1)] # interval tuples\n",
    "    \n",
    "    id_max_gs = np.argmax(gs)\n",
    "    id_max_pelt_segment = np.where(change>id_max_gs)[0][0]\n",
    "\n",
    "\n",
    "    # If the max gradient is too early, the onset is probably not available\n",
    "    if id_max_gs < prediction_range:\n",
    "        onset=0\n",
    "        case=0\n",
    "\n",
    "    # If the onset happens at the beggining of the data. There is space before max_grad, but no prediction samples\n",
    "    elif id_max_pelt_segment==0:\n",
    "        onset = 0\n",
    "        case = 1\n",
    "\n",
    "    else:\n",
    "        onset = change[id_max_pelt_segment-1]\n",
    "        \n",
    "        # check if previous interval has grad > threshold\n",
    "        # For example, it goes down quickly before going up even more quickly. Pelt might separate two segments.\n",
    "        prev_start = change_tuples[id_max_pelt_segment-1][0]\n",
    "        prev_end = change_tuples[id_max_pelt_segment-1][1]\n",
    "        if (gs[prev_start:prev_end]>threshold).any():\n",
    "            onset = prev_start\n",
    "        if onset == 0:\n",
    "            case = 1\n",
    "        else:\n",
    "            # If there are enough samples for clustering after onset (2) or not (3)\n",
    "            case = 2 if (onset+clustering_length)<=len(ts) else 3\n",
    "\n",
    "    df_onset = df.iloc[onset:min(len(ts),onset+clustering_length)].copy()\n",
    "    df_onset['onset'] = onset\n",
    "    df_onset['onset_case'] = case\n",
    "    return df_onset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['df_Trondheim_A1L2B', 'df_Kristiansand_A1L2B', 'df_Porsgrunn_A1L2B']\n",
    "datasets = ['df_Kristiansand_A1L2B']\n",
    "df_regions_all = []\n",
    "for dataset in datasets:\n",
    "    print(dataset)\n",
    "    df = pd.read_parquet(ROOT_DIR+f\"/data/interim/{dataset}_filtered.parq\")\n",
    "\n",
    "    df['grad'] = df.groupby('pid').smoothed.transform(np.gradient)\n",
    "    df['grad2'] = df.groupby('pid').grad.transform(np.gradient)\n",
    "    df['grad_abs'] = df.grad.abs()\n",
    "\n",
    "    print('grads done')\n",
    "\n",
    "    df['centered_on_gradmax'] = df['smoothed']\n",
    "    df = df.groupby('pid', group_keys=False).apply(center_on_gradmax)\n",
    "    df['grad_idmax'] = df.groupby('pid', as_index=False)['grad_abs'].transform(lambda x: np.argmax(x))\n",
    "\n",
    "    print('centering done')\n",
    "\n",
    "    threshold = 0.65\n",
    "\n",
    "    # Group by 'pid' and find the row with the maximum 'grad_abs' for each sensor\n",
    "    id_list = df.query('grad_abs>@threshold').pid.unique()\n",
    "    df_list = df[df.pid.isin(id_list)]\n",
    "    print(f'pixels with grad>threshold: {len(id_list)}')\n",
    "\n",
    "    # max_grad_idx = df_list.groupby('pid', as_index=False)['grad_abs'].agg(lambda x: np.argmax(x))\n",
    "    # # # Extract 91 timestamps around the maximum 'grad_abs' timestamp for each pixel\n",
    "    # ntimestamps = df_list.timestamp.nunique()\n",
    "    # df_regions = []\n",
    "    # for pid, id in zip(max_grad_idx.pid.values, max_grad_idx.grad_abs.values):\n",
    "    #     start, end, case = bounds_around_id(id, max_value=ntimestamps-1, samples_before=30, samples_after=60)\n",
    "    #     df_temp = df_list.query('pid==@pid').iloc[start:end+1].copy()\n",
    "    #     df_temp['grad_case'] = case\n",
    "    #     df_regions.append(df_temp)\n",
    "    # df_regions = pd.concat(df_regions)\n",
    "\n",
    "    df_regions = []\n",
    "    for pid in id_list:\n",
    "        df_regions.append(get_df_onset(df.query('pid==@pid'), threshold=threshold, clustering_length=120))\n",
    "\n",
    "    df_regions = pd.concat(df_regions)\n",
    "\n",
    "    # df_regions['centered_on_gradmax'] = df_regions['smoothed']\n",
    "    # def normalize_smoothed(group):\n",
    "    #     max_grad_abs_idx = group['grad_abs'].idxmax()  # Index of max 'grad_abs'\n",
    "    #     max_smoothed = group.at[max_grad_abs_idx, 'smoothed']  # Value of 'smoothed' at max 'grad_abs'\n",
    "    #     group['centered_on_gradmax'] -= max_smoothed  # Normalize 'smoothed'\n",
    "    #     return group\n",
    "\n",
    "    # df_regions = df_regions.groupby('pid', group_keys=False).apply(normalize_smoothed)\n",
    "\n",
    "    df_regions_all.append(df_regions)\n",
    "\n",
    "df_regions_all = pd.concat(df_regions_all).reset_index()\n",
    "\n",
    "display(df_regions_all.head())\n",
    "display(df_regions_all.groupby('pid', as_index=False).onset_case.min().onset_case.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = df.query('grad_abs>0.65').pid.unique()[4]\n",
    "df_plot = df.query('pid==@id')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,9))\n",
    "\n",
    "ax.plot(df_plot.timestamp, df_plot.smoothed, label='Displacement')\n",
    "ax.set_ylabel('Ground displacement [mm]')\n",
    "ax.set_xlabel('Timestamp')\n",
    "ax.legend(loc=2)\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(df_plot.timestamp, df_plot.grad, color='red')\n",
    "ax2.hlines(y=-0.65, xmin=df_plot.timestamp.min(), xmax=df_plot.timestamp.max(), linestyles='dashdot', color='red')\n",
    "ax2.set_ylabel('Gradient [mm/sample]')\n",
    "plt.xlim([df_plot.timestamp.min(), df_plot.timestamp.max()])\n",
    "ax.grid(axis='x')\n",
    "ax2.legend(['Gradient','Threshold'], loc=0)\n",
    "\n",
    "# plt.savefig(ROOT_DIR+f\"/models/outputs/figs/ReportESA/gradient.png\", transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_regions_all.to_parquet(ROOT_DIR+\"/data/interim/df_regions_all.parq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regions = pd.read_parquet(ROOT_DIR+\"/data/interim/df_regions_all.parq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regions.groupby('pid', as_index=False).grad_idmax.min().grad_idmax.hist(backend='plotly', nbins=500, cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(df_regions.groupby('pid', as_index=False).min(), x='onset', color='onset_case').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 4\n",
    "df_cluster = df_regions.query('onset_case==1 or onset_case==2').copy()\n",
    "y_pred = dtwkmeans(df_cluster, cluster_by='centered_on_gradmax', n_clusters=n_clusters, n_init=5)\n",
    "df_cluster['grad_cluster'] = np.repeat(y_pred, df_cluster.pid.value_counts()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster.to_parquet(ROOT_DIR+\"/data/interim/df_cluster_all.parq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster.query('onset_case==2').onset.hist(backend='plotly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 4\n",
    "df_cluster = df_regions.query('onset_case==1').copy()\n",
    "y_pred = dtwkmeans(df_cluster, cluster_by='centered_on_gradmax', n_clusters=n_clusters, n_init=5)\n",
    "df_cluster['grad_cluster'] = np.repeat(y_pred, df_cluster.pid.value_counts()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot = df_list[df_list.pid.isin(df_cluster.pid.unique())]\n",
    "df_plot = df_plot.merge(df_cluster.drop_duplicates('pid')[['pid','grad_cluster']], how='left', on='pid')\n",
    "df_plot['idmax'] = df_plot.groupby('pid').grad_abs.transform(lambda x: np.argmax(x))\n",
    "df_plot = df_plot.sort_values(['idmax','timestamp'])\n",
    "\n",
    "for cluster in range(n_clusters):\n",
    "    fig = px.line(df_plot.query('grad_cluster==@cluster'),\n",
    "                x='timestamp', y='smoothed', animation_frame='pid', hover_data=['grad'])\n",
    "    fig.update_yaxes(range=[-20, 20])  # Adjust the range as needed\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in range(n_clusters):\n",
    "    fig = px.line(df_cluster.groupby('pid', as_index=False)\n",
    "                            .apply(lambda x: x.reset_index(drop=True)).reset_index()\n",
    "                            .query('grad_cluster==@cluster'),\n",
    "                x='level_1', y='centered_on_gradmax', animation_frame='pid', hover_data=['grad'])\n",
    "    fig.update_yaxes(range=[-20, 20])  # Adjust the range as needed\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ruptures as rpt\n",
    "\n",
    "plot_ids = [51, 56, 59, 71]\n",
    "\n",
    "# for id in range(50,100):\n",
    "for id in plot_ids:\n",
    "\n",
    "    print(f'{id}-----------------------------------------------------------------------------------')\n",
    "    ts_plot = df_list[df_list.pid==id_list[id]].smoothed.values\n",
    "    ts_train = df_list[df_list.pid==id_list[id]].grad2.values\n",
    "\n",
    "    # pelt = rpt.Pelt(model='rbf', min_size=60).fit(np.diff(ts_train))\n",
    "    # pelt2 = rpt.Pelt(model='rbf', min_size=60).fit(np.diff(ts_plot))\n",
    "    # result = pelt.predict(pen=0.5)\n",
    "    # result2 = pelt2.predict(pen=1)\n",
    "\n",
    "    pelt = rpt.Pelt(model='rbf', min_size=60)\n",
    "    result = pelt.fit_predict(ts_plot, pen=1)\n",
    "\n",
    "    # Plot the change points\n",
    "    # rpt.display(ts_plot, result)\n",
    "    rpt.display(ts_plot, result)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ruptures as rpt\n",
    "\n",
    "plot_ids = [51, 56, 59, 71]\n",
    "\n",
    "\n",
    "count = 0\n",
    "for id in plot_ids:\n",
    "    count=count+1\n",
    "\n",
    "    ts_plot = df_list[df_list.pid==id_list[id]].smoothed.values\n",
    "    ts_train = df_list[df_list.pid==id_list[id]].grad2.values\n",
    "\n",
    "    pelt = rpt.Pelt(model='rbf', min_size=60)\n",
    "    result = pelt.fit_predict(ts_plot, pen=1)\n",
    "\n",
    "    # Plot the change points\n",
    "    # rpt.display(ts_plot, result)\n",
    "    rpt.display(ts_plot, result, figsize=(16,4))\n",
    "    plt.tick_params(\n",
    "        axis='both',          # changes apply to the x-axis\n",
    "        which='major',      # both major and minor ticks are affected\n",
    "        left = False,\n",
    "        labelleft = False,\n",
    "        bottom=False,      # ticks along the bottom edge are off\n",
    "        top=False,         # ticks along the top edge are off\n",
    "        labelbottom=False) # labels along the bottom edge are off\n",
    "    plt.box(False)\n",
    "    plt.savefig(ROOT_DIR+f\"/models/outputs/figs/ReportESA/pelt_{count}.png\", transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retf.add_subplot() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = fig[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figu, ax = plt.subplots()\n",
    "ax = f.get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO ALSO HIGH FREQUENCY FOR GRAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster = pd.read_parquet(ROOT_DIR+\"/data/interim/df_cluster_all.parq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dtwkmeans(df_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 4\n",
    "df_cluster_2 = df_cluster.query('grad_cluster==2 or grad_cluster==3').copy()\n",
    "y_pred = dtwkmeans(df_cluster_2, cluster_by='centered_on_gradmax', n_clusters=n_clusters, n_init=5)\n",
    "df_cluster_2['grad_cluster_2'] = np.repeat(y_pred, df_cluster.pid.value_counts()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot = df[df.pid.isin(df_cluster_2.pid.unique())]\n",
    "df_plot = df_plot.merge(df_cluster_2.drop_duplicates('pid')[['pid','grad_cluster_2']], how='left', on='pid')\n",
    "df_plot['idmax'] = df_plot.groupby('pid').grad_abs.transform(lambda x: np.argmax(x))\n",
    "df_plot = df_plot.sort_values(['idmax','timestamp'])\n",
    "\n",
    "for cluster in range(n_clusters):\n",
    "    fig = px.line(df_plot.query('grad_cluster_2==@cluster'),\n",
    "                x='timestamp', y='smoothed', animation_frame='pid', hover_data=['grad'])\n",
    "    fig.update_yaxes(range=[-20, 20])  # Adjust the range as needed\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shapelets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['df_Trondheim_A1L2B', 'df_Kristiansand_A1L2B', 'df_Porsgrunn_A1L2B']\n",
    "df = []\n",
    "for dataset in datasets:\n",
    "    df.append(pd.read_parquet(ROOT_DIR+f\"/data/interim/{dataset}_filtered.parq\"))\n",
    "\n",
    "df = pd.concat(df)\n",
    "\n",
    "df['grad'] = df.groupby('pid').smoothed.transform(np.gradient)\n",
    "df['grad2'] = df.groupby('pid').grad.transform(np.gradient)\n",
    "df['grad_abs'] = df.grad.abs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster = pd.read_parquet(ROOT_DIR+\"/data/interim/df_cluster_all.parq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dtwkmeans(df_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shape = df[df.pid.isin(df_cluster.query('onset_case==2').pid.unique())].copy()\n",
    "df_shape = df_shape.merge(df_cluster[['pid','grad_cluster', 'onset']].drop_duplicates(), how='left', on='pid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_upto_onset(group):\n",
    "    onset = group['onset'].min()  # Index of max 'grad_abs'\n",
    "    group = group.iloc[onset-60:onset]\n",
    "    return group\n",
    "\n",
    "# dft = df_shape.groupby('pid', as_index=False).apply(get_upto_onset).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shape = df[df.pid.isin(df_cluster.query('onset_case==2').pid.unique())].copy()\n",
    "df_shape = df_shape.merge(df_cluster[['pid','grad_cluster', 'onset']].drop_duplicates(), how='left', on='pid')\n",
    "df_shape = df_shape.groupby('pid', as_index=False).apply(get_upto_onset).reset_index(drop=True)\n",
    "\n",
    "column_X = 'smoothed'\n",
    "column_y = 'grad_cluster'\n",
    "X = np.array(df_shape[column_X].values.reshape((-1, df_shape.pid.value_counts()[0])))\n",
    "y = np.array(df_shape[column_y].values.reshape((-1, df_shape.pid.value_counts()[0]))[:,0].astype(int))\n",
    "\n",
    "X = TimeSeriesScalerMinMax().fit_transform(X).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of shapelets per size as done in the original paper\n",
    "shapelet_sizes = grabocka_params_to_shapelet_size_dict(n_ts=n_ts,\n",
    "                                                       ts_sz=ts_sz,\n",
    "                                                       n_classes=n_classes,\n",
    "                                                       l=0.1,\n",
    "                                                       r=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shp_clf = ShapeletTransformClassifier()\n",
    "shp_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = shp_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_pred, normalize='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster.groupby('pid', as_index=False).agg({'grad_abs':'max', 'grad_cluster':'min'}).groupby('grad_cluster').grad_abs.agg(['mean','std'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['df_Trondheim_A1L2B', 'df_Kristiansand_A1L2B', 'df_Porsgrunn_A1L2B']\n",
    "df = []\n",
    "for dataset in datasets:\n",
    "    df.append(pd.read_parquet(ROOT_DIR+f\"/data/interim/{dataset}_filtered.parq\"))\n",
    "\n",
    "df = pd.concat(df)\n",
    "\n",
    "df['grad'] = df.groupby('pid').smoothed.transform(np.gradient)\n",
    "df['grad2'] = df.groupby('pid').grad.transform(np.gradient)\n",
    "df['grad_abs'] = df.grad.abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy_list = df.groupby('pid',as_index=False).grad_abs.max().query('grad_abs<0.5').pid.unique()\n",
    "healthy_selected = healthy_list[np.random.randint(0, high=len(healthy_list),size=1500)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_healthy = df[df.pid.isin(healthy_selected)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_healthy_regions = []\n",
    "for pid in healthy_selected:\n",
    "    df_healthy_regions.append(get_df_onset(df_healthy.query('pid==@pid'), threshold=0.4, clustering_length=120))\n",
    "\n",
    "df_healthy_regions = pd.concat(df_healthy_regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_healthy_regions.onset_case.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regions = pd.read_parquet(ROOT_DIR+\"/data/interim/df_regions_all.parq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(df_test.groupby('pid', as_index=False).apply(lambda x: x.reset_index(drop=True)).reset_index(),\n",
    "        x='level_1', y='zeromean', animation_frame='pid', hover_data=['grad'])\n",
    "fig.update_yaxes(range=[-40, 40])  # Adjust the range as needed\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(df.groupby('pid',as_index=False).grad.agg(['max','min']).reset_index(), x='max', y='min').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('pid', as_index=False)['grad'].agg(lambda x: x.iloc[np.argmax(np.abs(x))]).grad.hist(backend='plotly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list = (\n",
    "            df\n",
    "            .groupby('pid', as_index=False)['grad']\n",
    "            .agg(lambda x: x.iloc[np.argmax(np.abs(x))])\n",
    "            .query('grad>0.7 or grad<-0.7')\n",
    "            .pid.unique()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dtwkmeans(df[df.pid.isin(id_list)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = df[df.pid.isin(id_list)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list['grad_cluster'] = np.repeat(y_pred, df_list.timestamp.nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(df_list.query('grad_cluster==0'), x='timestamp', y='smoothed', animation_frame='pid').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(df, x='grad', facet_row='grad_cluster').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('pid').grad.apply(lambda x: x.abs().max()).hist(backend='plotly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in range(0,50):\n",
    "    preproc = ['normalize',\n",
    "            'td',\n",
    "            # 'deseasonalize',\n",
    "            # 'detrend_linear'\n",
    "            ]\n",
    "    x = df_list[df_list.pid==df_list.pid.unique()[ind]].smoothed.values.reshape((1,-1))\n",
    "    regions1 = maxdiv(x, useLibMaxDiv=True,  preproc=preproc, method='gaussian_cov',\n",
    "                    extint_min_len=90, extint_max_len=90, num_intervals=1)\n",
    "    \n",
    "    regions2 = maxdiv(x, useLibMaxDiv=True,  preproc=preproc, method='parzen',\n",
    "                extint_min_len=90, extint_max_len=90, num_intervals=1)\n",
    "\n",
    "\n",
    "    fig = px.line(x.flatten())\n",
    "    for region in regions1:\n",
    "        fig.add_vline(x=region[0], line_color='green')\n",
    "        fig.add_vline(x=region[1], line_color='green')\n",
    "\n",
    "    for region in regions2:\n",
    "        fig.add_vline(x=region[0], line_color='red')\n",
    "        fig.add_vline(x=region[1], line_color='red')\n",
    "    # fig.update_layout(title=f\"{[r[2] for r in regions]}\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = maxdiv(df[df.pid==df.pid.unique()[0]].smoothed.values.reshape((1,-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df[['pid','smoothed','hf']].groupby('pid', as_index=False).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df_test.smoothed.values, df_test.hf.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mma = reload(mma)\n",
    "mma.relevant_neighborhood(df.query('latitude>63.4355 and longitude<10.41'), column_name='hf', lower=0, upper=40, zoom=11,\n",
    "                          only_relevant=False, filter_dates=False, by_max=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query('latitude>63.4355 and longitude<10.41').iloc[430520]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics.hf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_detection, selected = detection(df_metrics, column_name='hf', threshold_min=60, selector='pid',\n",
    "                                   detection_param='detection_sum', detection_param_threshold=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mma = reload(mma)\n",
    "mma.relevant_neighborhood(df, column_name='hf', lower=0, zoom=11, range_meters=20,\n",
    "                          only_relevant=False, filter_dates=False, by_max=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dario-juiScTYW-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
